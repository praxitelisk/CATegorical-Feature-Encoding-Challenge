{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CATegorical Feature Encoding Challenge EDA + ML"},{"metadata":{},"cell_type":"markdown","source":"![](https://st3.depositphotos.com/1051435/12643/i/950/depositphotos_126432526-stock-photo-a-group-of-cats-sitting.jpg)\n[image-source](https://st3.depositphotos.com/1051435/12643/i/950/depositphotos_126432526-stock-photo-a-group-of-cats-sitting.jpg)"},{"metadata":{},"cell_type":"markdown","source":"In this competition, you will be predicting the probability [0, 1] of a binary target column.\n\nThe data contains binary features (bin_*), nominal features (nom_*), ordinal features (ord_*) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters."},{"metadata":{},"cell_type":"markdown","source":"# Loading main libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/cat-in-the-dat/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/cat-in-the-dat/test.csv\")\nsub_df = pd.read_csv(\"/kaggle/input/cat-in-the-dat/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA\nExploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"![](https://i5.walmartimages.com/asr/9167cad6-5213-4052-b09a-e81d9fdc313b_1.50035d03658475373548d614f6825c4a.jpeg?odnHeight=450&odnWidth=450&odnBg=FFFFFF)\n[image-source](https://i5.walmartimages.com/asr/9167cad6-5213-4052-b09a-e81d9fdc313b_1.50035d03658475373548d614f6825c4a.jpeg?odnHeight=450&odnWidth=450&odnBg=FFFFFF)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check for NA missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing values which is great!"},{"metadata":{},"cell_type":"markdown","source":"# The target feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.round(train_df.target.value_counts()[0]/train_df.target.value_counts()[1],5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Univariate - Exploring the bin features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']:\n    print(col)\n    print(train_df[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']:\n    \n    f, ax = plt.subplots(1, 1, figsize=(10,5))\n    \n    sns.set(style=\"white\", context=\"talk\")\n    \n    sns.countplot(x=col, data=train_df, ax=ax, palette=\"Set1\")\n    f.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Univariate - Exploring the nominal features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6','nom_7', 'nom_8']:\n    print(col)\n    print(train_df[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8']:\n    \n    f, ax = plt.subplots(1, 1, figsize=(10,5))\n    \n    \n    sns.countplot(x=col, data=train_df, ax=ax, palette=\"Set1\")\n    f.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that high cardinality nominal deatures cannot be depicted with seaborn due to the fact that there are many categories."},{"metadata":{},"cell_type":"markdown","source":"# Univariate - Exploring the Ordinal Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]:\n    print(col)\n    print(train_df[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]:\n    \n    f, ax = plt.subplots(1, 1, figsize=(10,5))\n    \n    sns.countplot(x=col, data=train_df, ax=ax, palette=\"Set1\")\n    f.tight_layout()\n    \nimport gc\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering for Binary features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.loc[train_df['bin_3'] == 'T', 'bin_3'] = 1\ntrain_df.loc[train_df['bin_3'] == 'F', 'bin_3'] = 0\n\ntest_df.loc[test_df['bin_3'] == 'T', 'bin_3'] = 1\ntest_df.loc[test_df['bin_3'] == 'F', 'bin_3'] = 0\n\ntrain_df.loc[train_df['bin_4'] == 'Y', 'bin_4'] = 1\ntrain_df.loc[train_df['bin_4'] == 'N', 'bin_4'] = 0\n\ntest_df.loc[test_df['bin_4'] == 'Y', 'bin_4'] = 1\ntest_df.loc[test_df['bin_4'] == 'N', 'bin_4'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering for Nominal Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ncolumns = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n\nfor col in columns:\n    \n    le.fit(train_df[col].to_list()+test_df[col].to_list())\n    train_df[col] = le.transform(train_df[col])\n    test_df[col] = le.transform(test_df[col])\n    \nimport gc\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering for Ordinal Features\nI will use the one hot encoding method"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport gc\n\nfor col in [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]:\n    \n    print(col, \"one-hot-encoding\")\n    le = LabelEncoder()\n    train_df[col] = le.fit_transform(train_df[col])\n    test_df[col] = le.transform(test_df[col])\n\n    \n    ohe = OneHotEncoder()\n    train_col = ohe.fit_transform(train_df[col].values.reshape(-1,1)).toarray()\n    test_col = ohe.fit_transform(test_df[col].values.reshape(-1,1)).toarray()\n\n    dfOneHot = pd.DataFrame(train_col, columns = [col+\".\"+str(int(i)) for i in range(train_col.shape[1])])\n    train_df = pd.concat([train_df, dfOneHot], axis=1)\n\n    dfOneHot = pd.DataFrame(test_col, columns = [col+\".\"+str(int(i)) for i in range(test_col.shape[1])])\n    test_df = pd.concat([test_df, dfOneHot], axis=1)\n\n    train_df.drop(col, axis=1, inplace=True);\n    test_df.drop(col, axis=1, inplace=True);\n    \n    del train_col; del test_col; del dfOneHot;\n    gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Training"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.zirous.com/wp-content/uploads/2019/01/Word-Cloud-02.png)\n[image-source](https://www.zirous.com/wp-content/uploads/2019/01/Word-Cloud-02.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_columns = train_df.columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for elem in [\"id\",\"target\"]:\n    train_columns.remove(elem)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['target']\nX = train_df.drop(['target'], axis=1)\nX = X[train_columns]\n\nclf_stats_df = pd.DataFrame(columns=[\"clf_name\", \"F1-score\", \"auc-score\"])  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost with default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport seaborn as sns\nimport scikitplot as skplt\n\n# create a 80/20 split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\npredictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_xgb = np.zeros(len(test_df))\nnum_of_folds = 2\nnum_fold = 0\n    #feature_importance_df = pd.DataFrame()\n\nfolds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Stratified Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_xgb = xgb.XGBClassifier(n_estimators = 4000,\n                                     objective= 'binary:logistic',\n                                     nthread=-1,\n                                     seed=42)\n\n    clf_stra_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=100, eval_metric='auc', verbose=250)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions = clf_stra_xgb.predict(xvalid)\n    predictions_probas = clf_stra_xgb.predict_proba(xvalid)\n    predictions_probas_list += predictions_probas/num_of_folds\n\n    predictions_test += clf_stra_xgb.predict_proba(test_df[xtrain.columns])[:,1]/num_of_folds\n\n\npredictions = np.argmax(predictions_probas, axis=1)\n\nprint()\nprint(classification_report(yvalid, predictions))\n\nprint()\nprint(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\nprint()\nprint(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\"))\n\nprint()\nprint(\"elapsed time in seconds: \", time.time() - start_time)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_roc(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(12, 38)})\nxgb.plot_importance(clf_stra_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\nclf_stats_df = clf_stats_df.append({\"clf_name\": \"clf_stra_xgb\",\n                     \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                     \"auc-score\": roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\")}, ignore_index=True)\n\nprint()\nimport gc\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# HyperOpt Tuning for XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n\nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.2)\n\n# https://github.com/hyperopt/hyperopt/wiki/FMin\n\ndef objective(space):\n\n    clf = xgb.XGBClassifier(n_estimators = 2000,\n                            max_depth = int(space['max_depth']),\n                            min_child_weight = space['min_child_weight'],\n                            subsample = space['subsample'],\n                            gamma=space['gamma'],\n                            colsample_bytree=space['colsample_bytree'],\n                            reg_alpha = space['reg_alpha'],\n                            reg_lambda = space['reg_lambda'],\n                            scale_pos_weight = space[\"scale_pos_weight\"],\n                            objective= 'binary:logistic',\n                            nthread=-1,\n                            seed=42)\n\n    eval_set  = [( xtrain, ytrain), ( xvalid, yvalid)]\n\n    clf.fit(xtrain, ytrain,\n            eval_set=eval_set, eval_metric=\"auc\",\n            early_stopping_rounds=100, verbose=250)\n\n    pred = clf.predict_proba(xvalid)[:,1]\n    auc = roc_auc_score(yvalid, pred)\n    print(\"SCORE:\", auc)\n\n    return{'loss':1-auc, 'status': STATUS_OK }\n\n# https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster\nspace ={\n        'eta': hp.uniform('eta', 0.025, 0.5),\n        'max_depth': hp.uniform('max_depth', 3, 12),\n        'min_child_weight': hp.quniform ('min_child_weight', 1, 10, 1),\n        'subsample': hp.uniform ('subsample', 0.5, 1),\n        'gamma': hp.uniform('gamma', 0.1, 1),\n        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n        'reg_alpha': hp.uniform ('reg_alpha', 0, 1),\n        'reg_lambda': hp.uniform ('reg_lambda', 0, 10),\n        'scale_pos_weight': hp.uniform(\"scale_pos_weight\", 1, np.round(train_df.target.value_counts()[0]/train_df.target.value_counts()[1],5))\n    }\n\n\ntrials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=2,\n            trials=trials,\n            verbose = 0)\n\nprint(best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost Training after Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport seaborn as sns\nimport scikitplot as skplt\n\n# create a 80/20 split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\npredictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_tuned_xgb = np.zeros(len(test_df))\nnum_of_folds = 2\nnum_fold = 0\n    #feature_importance_df = pd.DataFrame()\n\nfolds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Stratified Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_tuned_xgb = xgb.XGBClassifier(n_estimators = 4000,\n                                           objective= 'binary:logistic',\n                                           nthread=-1,\n                                           eta = best['eta'],\n                                           max_depth=int(best['max_depth']),\n                                           min_child_weight=best['min_child_weight'],\n                                           subsample=best['subsample'],\n                                           gamma=best['gamma'],\n                                           colsample_bytree=best['colsample_bytree'],\n                                           reg_alpha = best['reg_alpha'],\n                                           reg_lambda = best['reg_lambda'],\n                                           scale_pos_weight = best['scale_pos_weight'],\n                                           seed=42)\n    \n\n    clf_stra_tuned_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=100, eval_metric='auc', verbose=250)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions = clf_stra_tuned_xgb.predict(xvalid)\n    predictions_probas = clf_stra_tuned_xgb.predict_proba(xvalid)\n    predictions_probas_list += predictions_probas/num_of_folds\n\n    predictions_test_tuned_xgb += clf_stra_tuned_xgb.predict_proba(test_df[xtrain.columns])[:,1]/num_of_folds\n\n\npredictions = np.argmax(predictions_probas, axis=1)\n\nprint()\nprint(classification_report(yvalid, predictions))\n\nprint()\nprint(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\nprint()\nprint(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\"))\n\nprint()\nprint(\"elapsed time in seconds: \", time.time() - start_time)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_roc(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(12, 38)})\nxgb.plot_importance(clf_stra_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\nclf_stats_df = clf_stats_df.append({\"clf_name\": \"clf_tuned_xgb\",\n                     \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                     \"auc-score\": roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\")}, ignore_index=True)\n\nprint()\nimport gc\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dealing the class Imbalance\n- Dealing class imbalance with SMOTE and training xgboost with default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nX_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X, y)\nX_resampled = pd.DataFrame(X_resampled, columns= X.columns)\ny_resampled = pd.Series(y_resampled)\n\nimport gc\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Class Imbalance and XGBoost with default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport seaborn as sns\nimport scikitplot as skplt\n\n# create a 80/20 split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X_resampled, y_resampled, random_state=42, test_size=0.2, stratified=y_resampled)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\npredictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_smote_xgb = np.zeros(len(test_df))\nnum_of_folds = 2\nnum_fold = 0\n    #feature_importance_df = pd.DataFrame()\n\nfolds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Stratified Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_smote_xgb = xgb.XGBClassifier(n_estimators = 4000,\n                                     objective= 'binary:logistic',\n                                     nthread=-1,\n                                     seed=42)\n\n    clf_stra_smote_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=100, eval_metric='auc', verbose=100)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions = clf_stra_smote_xgb.predict(xvalid)\n    predictions_probas = clf_stra_smote_xgb.predict_proba(xvalid)\n    predictions_probas_list += predictions_probas/num_of_folds\n\n    predictions_test += clf_stra_smote_xgb.predict_proba(test_df[xtrain.columns])[:,1]/num_of_folds\n\n\npredictions = np.argmax(predictions_probas, axis=1)\n\nprint()\nprint(classification_report(yvalid, predictions))\n\nprint()\nprint(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\nprint()\nprint(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\"))\n\nprint()\nprint(\"elapsed time in seconds: \", time.time() - start_time)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_roc(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(12, 38)})\nxgb.plot_importance(clf_stra_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\nclf_stats_df = clf_stats_df.append({\"clf_name\": \"clf_stra_smote_xgb\",\n                     \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                     \"auc-score\": roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\")}, ignore_index=True)\n\nprint()\nimport gc\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_resampled;\ndel y_resampled;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest with default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport seaborn as sns\nimport scikitplot as skplt\n\n# create a 80/20 split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nstart_time = time.time()\n\npredictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_random_forest = np.zeros(len(test_df))\nnum_of_folds = 2\nnum_fold = 0\n    #feature_importance_df = pd.DataFrame()\n\nfolds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Stratified Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_random_forest = RandomForestClassifier(n_estimators = 100,\n                                     n_jobs=-1,\n                                     random_state=42)\n\n    clf_stra_random_forest.fit(xtrain_stra, ytrain_stra)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions = clf_stra_random_forest.predict(xvalid)\n    predictions_probas = clf_stra_random_forest.predict_proba(xvalid)\n    predictions_probas_list += predictions_probas/num_of_folds\n\n    predictions_test_random_forest += clf_stra_random_forest.predict_proba(test_df[xtrain.columns])[:,1]/num_of_folds\n\n\npredictions = np.argmax(predictions_probas, axis=1)\n\nprint()\nprint(classification_report(yvalid, predictions))\n\nprint()\nprint(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\nprint()\nprint(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\"))\n\nprint()\nprint(\"elapsed time in seconds: \", time.time() - start_time)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_roc(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n\n# sns.set(rc={'figure.figsize':(12, 38)})\n# xgb.plot_importance(clf_stra_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\nclf_stats_df = clf_stats_df.append({\"clf_name\": \"clf_stra_random_forest\",\n                     \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                     \"auc-score\": roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\")}, ignore_index=True)\n\nprint()\nimport gc\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n\nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.2)\n\n# https://github.com/hyperopt/hyperopt/wiki/FMin\n\ndef objective(space):\n\n    clf = RandomForestClassifier(n_estimators = space['n_estimators'],\n                                 criterion = space['criterion'],\n                                 max_features = space['max_features'],\n                                 max_depth = space['max_depth'],\n                                 n_jobs=-1,\n                                 random_state=42)\n\n\n    clf.fit(xtrain, ytrain)\n\n    pred = clf.predict_proba(xvalid)[:,1]\n    auc = roc_auc_score(yvalid, pred)\n    print(\"SCORE:\", auc)\n\n    return{'loss':1-auc, 'status': STATUS_OK }\n\n# https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster\nspace = {\n    'max_depth': hp.choice('max_depth', range(1,20)),\n    'max_features': hp.choice('max_features', range(1,150)),\n    'n_estimators': hp.choice('n_estimators', range(100,500)),\n    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n}\n\n\ntrials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=2,\n            trials=trials,\n            verbose = 0)\n\nprint(best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest after Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport seaborn as sns\nimport scikitplot as skplt\n\n# create a 80/20 split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nstart_time = time.time()\n\npredictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_tuned_random_forest = np.zeros(len(test_df))\nnum_of_folds = 2\nnum_fold = 0\n    #feature_importance_df = pd.DataFrame()\n\nfolds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Stratified Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_random_forest_tuned = RandomForestClassifier(n_estimators = best[\"n_estimators\"],\n                                                          max_features = best[\"max_features\"],\n                                                          max_depth = best[\"max_depth\"],\n                                                          criterion = [\"gini\", \"entropy\"][best[\"criterion\"]],\n                                                          n_jobs=-1,\n                                                          random_state=42)\n\n    clf_stra_random_forest_tuned.fit(xtrain_stra, ytrain_stra)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions = clf_stra_random_forest_tuned.predict(xvalid)\n    predictions_probas = clf_stra_random_forest_tuned.predict_proba(xvalid)\n    predictions_probas_list += predictions_probas/num_of_folds\n\n    predictions_test += clf_stra_random_forest_tuned.predict_proba(test_df[xtrain.columns])[:,1]/num_of_folds\n\n\npredictions = np.argmax(predictions_probas, axis=1)\n\nprint()\nprint(classification_report(yvalid, predictions))\n\nprint()\nprint(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\nprint()\nprint(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\"))\n\nprint()\nprint(\"elapsed time in seconds: \", time.time() - start_time)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_roc(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n\n# sns.set(rc={'figure.figsize':(12, 38)})\n# xgb.plot_importance(clf_stra_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\nclf_stats_df = clf_stats_df.append({\"clf_name\": \"clf_stra_random_forest_tuned\",\n                     \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                     \"auc-score\": roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\")}, ignore_index=True)\n\nprint()\nimport gc\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inspecting all classifiers and their performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_stats_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df['target'] = predictions_test_xgb\nsub_df.to_csv('clf_xgboost.csv', index=False)\n\nsub_df['target'] = predictions_test_tuned_xgb\nsub_df.to_csv('clf_xgboost_tuned.csv', index=False)\n\nsub_df['target'] = predictions_test_smote_xgb\nsub_df.to_csv('clf_xgboost_smote.csv', index=False)\n\nsub_df['target'] = predictions_test_random_forest\nsub_df.to_csv('clf_random_forest.csv', index=False)\n\nsub_df['target'] = predictions_test_tuned_random_forest\nsub_df.to_csv('clf_random_forest_tuned.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Thank you very much for checking out my kernel! Please upvote if you found it helpful or leave a comment if you have any suggestions for improvements."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}